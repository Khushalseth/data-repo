{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":939937,"sourceType":"datasetVersion","datasetId":501529},{"sourceId":956177,"sourceType":"datasetVersion","datasetId":519884},{"sourceId":6037307,"sourceType":"datasetVersion","datasetId":3382782},{"sourceId":8436676,"sourceType":"datasetVersion","datasetId":5025129},{"sourceId":8436787,"sourceType":"datasetVersion","datasetId":5025211},{"sourceId":504,"sourceType":"modelInstanceVersion","modelInstanceId":381,"modelId":34}],"dockerImageVersionId":30498,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:25:01.857948Z","iopub.execute_input":"2024-11-20T15:25:01.860181Z","iopub.status.idle":"2024-11-20T15:25:01.872497Z","shell.execute_reply.started":"2024-11-20T15:25:01.860132Z","shell.execute_reply":"2024-11-20T15:25:01.871596Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport seaborn as sb\nimport torch \nfrom torchvision import models\nfrom torch import nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader \nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights\nfrom torchvision.utils import make_grid\nfrom torch.autograd import Variable\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\nimport shutil  \nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ncode_dir = \"/kaggle/working/code\"\nmodel_dir = \"/kaggle/working/model\"\noutput_dir = \"/kaggle/working/output\"\n\nif not os.path.exists(code_dir):\n    os.mkdir(code_dir)\n\nif not os.path.exists(model_dir):\n    os.mkdir(model_dir)\n\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)\n    \nshutil.copyfile(src=\"/kaggle/input/modelos/convnext.py\", \n                dst=\"/kaggle/working/code/convnext.py\")\nshutil.copyfile(src=\"/kaggle/input/modelos/convnext_tiny_1k_224_ema.pth\", \n                dst=\"/kaggle/working/model/convnext_tiny_1k_224_ema.pth\")\nshutil.copyfile(src=\"/kaggle/input/modelos/vit_b_16-c867db91.pth\", \n                dst=\"/kaggle/working/model/vit_b_16-c867db91.pth\")\n\nos.chdir(\"/kaggle/working/code\")\n\n\nfrom convnext import ConvNeXt\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:25:27.229768Z","iopub.execute_input":"2024-11-20T15:25:27.230659Z","iopub.status.idle":"2024-11-20T15:25:28.056930Z","shell.execute_reply.started":"2024-11-20T15:25:27.230627Z","shell.execute_reply":"2024-11-20T15:25:28.055950Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# **2. Database**","metadata":{}},{"cell_type":"code","source":"def ConvNeXt_model():\n    model_conv=ConvNeXt()\n    state_dict = torch.load('/kaggle/working/model/convnext_tiny_1k_224_ema.pth')\n    model_conv.load_state_dict(state_dict[\"model\"])\n    \n    return model_conv\n\ndef ViT_model():\n    model_vit=vit_b_16(pretrained=True)\n    return model_vit\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:25:34.135978Z","iopub.execute_input":"2024-11-20T15:25:34.136927Z","iopub.status.idle":"2024-11-20T15:25:34.141356Z","shell.execute_reply.started":"2024-11-20T15:25:34.136894Z","shell.execute_reply":"2024-11-20T15:25:34.140335Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:25:37.420566Z","iopub.execute_input":"2024-11-20T15:25:37.420910Z","iopub.status.idle":"2024-11-20T15:25:37.488324Z","shell.execute_reply.started":"2024-11-20T15:25:37.420883Z","shell.execute_reply":"2024-11-20T15:25:37.487186Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"  criterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:25:39.715718Z","iopub.execute_input":"2024-11-20T15:25:39.716044Z","iopub.status.idle":"2024-11-20T15:25:39.720754Z","shell.execute_reply.started":"2024-11-20T15:25:39.716018Z","shell.execute_reply":"2024-11-20T15:25:39.719706Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:25:43.346221Z","iopub.execute_input":"2024-11-20T15:25:43.346957Z","iopub.status.idle":"2024-11-20T15:25:43.350845Z","shell.execute_reply.started":"2024-11-20T15:25:43.346924Z","shell.execute_reply":"2024-11-20T15:25:43.350014Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_acc, teste_acc, train_loss, teste_loss = [], [], [], []\ntrain_precision, teste_precision, train_recall, teste_recall = [], [], [], []\ntrain_f1, teste_f1 = [], []\ndf = pd.DataFrame(columns=['Modelo','Experimento','Epoch', 'Train ACC', 'Train Loss', 'Train F1', 'Test ACC', 'Test Loss', 'Test F1'])","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:25:45.576467Z","iopub.execute_input":"2024-11-20T15:25:45.576779Z","iopub.status.idle":"2024-11-20T15:25:45.587604Z","shell.execute_reply.started":"2024-11-20T15:25:45.576758Z","shell.execute_reply":"2024-11-20T15:25:45.586691Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### **3.1.1. Image Processing**","metadata":{}},{"cell_type":"code","source":"def full_data_transform(model_type, data_fraction, batch_size):\n\n    if model_type== 'convnext':\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(256),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    elif model_type == 'vit':\n        transform = ViT_B_16_Weights.IMAGENET1K_V1.transforms()\n            \n            \n    local_arquivos='/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'\n    full_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=transform)\n    full_test_dataset = ImageFolder(local_arquivos + \"/test\", transform=transform)\n            \n   \n    num_train_data = int(len(full_train_dataset) * data_fraction)\n    num_test_data = int(len(full_test_dataset) * data_fraction)\n    \n\n    train_indices = random.sample(range(len(full_train_dataset)), num_train_data)\n    test_indices = random.sample(range(len(full_test_dataset)), num_test_data)\n\n    train_dataset = torch.utils.data.Subset(full_train_dataset, train_indices)\n    test_dataset = torch.utils.data.Subset(full_test_dataset, test_indices)\n    \n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_dataloader = DataLoader(test_dataset, batch_size=int(batch_size/2), shuffle=False)\n        \n        \n    return train_dataloader, test_dataloader\n\ndef ft_data_transform(model_type, data_fraction, batch_size):\n\n    if model_type== 'convnext':\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(256),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n    \n    elif model_type == 'vit':\n        transform = ViT_B_16_Weights.IMAGENET1K_V1.transforms()\n\n    local_arquivos='/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'\n    full_test_dataset = ImageFolder(local_arquivos + \"/test\", transform=transform)\n            \n    num_test_data = int(len(full_test_dataset) * data_fraction)\n    \n    test_indices = random.sample(range(len(full_test_dataset)), num_test_data)\n\n    test_dataset = torch.utils.data.Subset(full_test_dataset, test_indices)\n    \n    test_dataloader = DataLoader(test_dataset, batch_size=int(batch_size/2), shuffle=False)\n\n    return test_dataloader","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:25:48.948184Z","iopub.execute_input":"2024-11-20T15:25:48.948838Z","iopub.status.idle":"2024-11-20T15:25:48.959551Z","shell.execute_reply.started":"2024-11-20T15:25:48.948809Z","shell.execute_reply":"2024-11-20T15:25:48.958650Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### **3.1.2. AutoAugment**","metadata":{}},{"cell_type":"code","source":"def AutoAugment_transform(model_type, train_indices, batch_size):\n    if model_type== 'convnext':\n        augmentation_transforms = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(256),\n            transforms.AutoAugment(), \n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n    elif model_type == 'vit':\n        augmentation_transforms = transforms.Compose([\n            transforms.Resize(224, interpolation=Image.BILINEAR),\n            transforms.CenterCrop(224),\n            transforms.AutoAugment(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n        ])\n    local_arquivos='/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'\n    augmented_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=augmentation_transforms)\n    augmented_train_dataset = torch.utils.data.Subset(augmented_train_dataset, train_indices)\n    augmented_train_dataloader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n    \n    return augmented_train_dataloader\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:25:55.131692Z","iopub.execute_input":"2024-11-20T15:25:55.132008Z","iopub.status.idle":"2024-11-20T15:25:55.138921Z","shell.execute_reply.started":"2024-11-20T15:25:55.131983Z","shell.execute_reply":"2024-11-20T15:25:55.138065Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### **3.1.3. RandAugment**","metadata":{}},{"cell_type":"code","source":"def RandAugment_transform(model_type, train_indices, batch_size):\n    if model_type== 'convnext':\n        augmentation_transforms = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(256),\n            transforms.RandAugment(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    elif model_type == 'vit':\n        augmentation_transforms = transforms.Compose([\n            transforms.Resize(224, interpolation=Image.BILINEAR),\n            transforms.CenterCrop(224),\n            transforms.RandAugment(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n        ])\n        \n    local_arquivos='/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'\n    augmented_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=augmentation_transforms)\n    augmented_train_dataset = torch.utils.data.Subset(augmented_train_dataset, train_indices)\n    augmented_train_dataloader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n    \n    return augmented_train_dataloader\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:26:00.612518Z","iopub.execute_input":"2024-11-20T15:26:00.613408Z","iopub.status.idle":"2024-11-20T15:26:00.619749Z","shell.execute_reply.started":"2024-11-20T15:26:00.613345Z","shell.execute_reply":"2024-11-20T15:26:00.618813Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### **3.1.4. Auto+Rand Augment**","metadata":{}},{"cell_type":"code","source":"def Auto_RandAugment_transform(model_type, train_indices, batch_size):\n    if model_type== 'convnext':\n        augmentation_transforms = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(256),\n            transforms.AutoAugment(),\n            transforms.RandAugment(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    elif model_type == 'vit':\n        augmentation_transforms = transforms.Compose([\n            transforms.Resize(224, interpolation=Image.BILINEAR),\n            transforms.CenterCrop(224),\n            transforms.AutoAugment(),\n            transforms.RandAugment(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n        ])\n        \n    local_arquivos='/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'\n    augmented_train_dataset = ImageFolder(local_arquivos + \"/train\", transform=augmentation_transforms)\n    augmented_train_dataset = torch.utils.data.Subset(augmented_train_dataset, train_indices)\n    augmented_train_dataloader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n    \n    return augmented_train_dataloader","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:26:04.579544Z","iopub.execute_input":"2024-11-20T15:26:04.579864Z","iopub.status.idle":"2024-11-20T15:26:04.586676Z","shell.execute_reply.started":"2024-11-20T15:26:04.579840Z","shell.execute_reply":"2024-11-20T15:26:04.585754Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## **3.2. Training and Testing**","metadata":{}},{"cell_type":"code","source":"\ndef train(model, dataloader, criterion, optimizer, scheduler, device, ft, epoch, exp, model_type):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    y_true,y_pred=[], []\n    \n    loop = tqdm(enumerate(dataloader), total=len(dataloader))\n    for batch_idx, (images, labels) in loop:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad() \n        if ft==False:    \n            for param in model.parameters():\n                param.requires_grad=False   \n                \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            optimizer.step()\n            \n            for param in model.parameters():\n                param.requires_grad=True\n            \n        else:    \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n                \n        running_loss += loss.item()\n        predicted = outputs.argmax(dim = 1)\n    \n        y_true.extend(labels.cpu().tolist())\n        y_pred.extend(predicted.cpu().tolist())\n\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n        \n        loop.set_description(f\"[Epoch {(epoch+1)}]\")\n        loop.set_postfix(loss=loss.item())\n        \n    if scheduler:\n        scheduler.step()\n        \n    train_loss = running_loss / len(dataloader.dataset)  \n    accuracy = accuracy_score(y_true, y_pred)\n    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro',zero_division=0)\n\n    print(f\"Train Loss: {train_loss:.6f} | Train Accuracy: {(accuracy * 100):.2f}% | Train F1-Score: {f1:.6f}\")\n    \n    model_name= f'model_{model_type}_params_exp_{exp}.pth'\n    torch.save(model.state_dict(), os.path.join('/kaggle/working/model', model_name))\n   \n    return train_loss, accuracy, f1","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:26:08.518028Z","iopub.execute_input":"2024-11-20T15:26:08.518811Z","iopub.status.idle":"2024-11-20T15:26:08.527747Z","shell.execute_reply.started":"2024-11-20T15:26:08.518783Z","shell.execute_reply":"2024-11-20T15:26:08.526731Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Generic testing function\ndef test(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    y_pred, y_true= [], []\n\n    with torch.no_grad():\n        for images, labels in dataloader:\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n\n            running_loss += loss.item()\n            predicted = outputs.argmax(dim = 1)\n            \n            y_true.extend(labels.cpu().tolist())\n            y_pred.extend(predicted.cpu().tolist())\n            \n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n    test_loss = running_loss / len(list(dataloader.dataset))\n    test_accuracy = accuracy = accuracy_score(y_true, y_pred)\n    precision, recall, test_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro',zero_division=0)\n    \n    print(f\"Test Loss: {test_loss:.6f} | Test Accuracy: {(test_accuracy * 100):.2f}% | Test F1-Score: {test_f1:.6f}\")\n    return test_loss, test_accuracy, test_f1\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:26:16.090407Z","iopub.execute_input":"2024-11-20T15:26:16.091231Z","iopub.status.idle":"2024-11-20T15:26:16.097959Z","shell.execute_reply.started":"2024-11-20T15:26:16.091184Z","shell.execute_reply":"2024-11-20T15:26:16.097068Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\ndef train_model(model_type, exp,model, train_dataloader, test_dataloader, criterion, optimizer, scheduler, device, num_epochs, ft, num):\n    model=model.to(device)\n    train_losses = []\n    train_accuracies = []\n    test_losses = []\n    test_accuracies = []\n\n    for epoch in range(num_epochs):\n        print('----------------------------------------------------------------------------')\n        train_loss, train_accuracy, train_f1 = train(model, train_dataloader, criterion, optimizer, scheduler, device, ft, epoch, exp, model_type)\n        test_loss, test_accuracy,test_f1 = test(model, test_dataloader, criterion, device)\n        val=str(num)+str(epoch+1)\n        df.loc[val]=[model_type, exp, epoch+1, train_accuracy, train_loss, train_f1, test_accuracy, test_loss, test_f1]\n        df.to_csv('metricas.csv', index = False)\n        print('\\n')\n        \n        \n        \n    return train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:26:17.995509Z","iopub.execute_input":"2024-11-20T15:26:17.995845Z","iopub.status.idle":"2024-11-20T15:26:18.002132Z","shell.execute_reply.started":"2024-11-20T15:26:17.995818Z","shell.execute_reply":"2024-11-20T15:26:18.001260Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def run_scenario(model_type, scenario, data_fraction, num_epochs=10, batch_size=32, learning_rate=0.001):\n    num_classes = 2  \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    num_train_data = int((100000) * data_fraction)\n    num_test_data = int((20000) * data_fraction)\n    \n    train_indices = random.sample(range(65000), num_train_data)\n    test_indices = random.sample(range(20000), num_test_data)\n    \n    print(f'Number of training objects: {num_train_data}')\n    print(f'Number of test objects: {num_test_data}')\n    print(\"============================================================================\")\n    \n    if scenario == 1:\n        \n        train_dataloader, test_dataloader= full_data_transform(model_type, data_fraction, batch_size)\n        \n        \n        if model_type == 'convnext':\n            model = ConvNeXt_model()\n            model.head = nn.Linear(model.head.in_features, num_classes)  # Substituir num_classes pelo número correto de classes\n            num=1\n\n        elif model_type == 'vit':\n            model = ViT_model()\n            model.heads=nn.Linear(768,2)\n            num=9\n        else:\n            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n        \n        model = model.to(device)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n        if model_type == 'convnext':\n            print(\"Experimento 1 - ConvNeXt (Sem Fine-Tuning)\")\n        elif model_type == 'vit':\n            print(\"Experimento 1 - ViT (Sem Fine-Tuning)\")\n        \n        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,1, model, train_dataloader, test_dataloader,\n                                                                                    criterion, optimizer, scheduler, device, 1, False, num)\n    elif scenario == 2:\n        \n        train_dataloader, test_dataloader= full_data_transform(model_type, data_fraction, batch_size)\n        \n       \n        if model_type == 'convnext':\n            model = ConvNeXt_model()\n            model.head = nn.Linear(model.head.in_features, num_classes)  # Substituir num_classes pelo número correto de classes\n            num=2\n\n        elif model_type == 'vit':\n            model = ViT_model()\n            model.heads=nn.Linear(768,2)\n            num=10\n        else:\n            raise ValueError(\"O parâmetro 'model_type' deve ser 'convnext' ou 'vit'.\")\n\n        model = model.to(device)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n        if model_type == 'convnext':\n             print(\"Experiment 2 - ConvNeXt (With Fine-Tuning)\")\n        elif model_type == 'vit':\n            print(\"Experiment 2 - ViT (With Fine-Tuning)\")\n        \n        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,2, model, train_dataloader, test_dataloader,\n                                                                                    criterion, optimizer, scheduler, device, num_epochs, True, num)\n    elif scenario == 3:\n        \n        augmented_train_dataloader = AutoAugment_transform(model_type, train_indices, batch_size)\n        test_dataloader=ft_data_transform(model_type, data_fraction, batch_size)\n        \n\n        if model_type == 'convnext':\n            model = ConvNeXt_model()\n            model.head = nn.Linear(model.head.in_features, num_classes)  \n            num=3\n\n        elif model_type == 'vit':\n            model = ViT_model()\n            model.heads=nn.Linear(768,2)\n            num=11\n        else:\n            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n\n        model = model.to(device)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n        if model_type == 'convnext':\n            print(\"Experiment 3 - ConvNeXt (No Fine-Tuning with AutoAugment\")\n        elif model_type == 'vit':\n            print(\"Experiment 3 - ViT (No Fine-Tuning with AutoAugment)\")\n        \n        \n        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,3, model, augmented_train_dataloader, test_dataloader,\n                                                                                   criterion, optimizer, scheduler, device, 1, False, num)\n      \n        if model_type == 'convnext':\n            model = ConvNeXt_model()\n            model.head = nn.Linear(model.head.in_features, num_classes)  \n            num=4\n            \n        elif model_type == 'vit':\n            model = ViT_model()\n            model.heads=nn.Linear(768,2)\n            num=12\n        else:\n            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n\n        model = model.to(device)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n        if model_type == 'convnext':\n            print(\"Experiment 4 - ConvNeXt (With Fine-Tuning and with AutoAugment)\")\n        elif model_type == 'vit':\n            print(\"Experiment 4 - ViT (With Fine-Tuning and with AutoAugment)\")\n        \n        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,4, model, augmented_train_dataloader, test_dataloader,\n                                                                                   criterion, optimizer, scheduler, device, num_epochs, True, num)\n    \n    elif scenario == 4:\n        augmented_train_dataloader = RandAugment_transform(model_type, train_indices, batch_size)\n        test_dataloader=ft_data_transform(model_type, data_fraction, batch_size)\n        \n        if model_type == 'convnext':\n            model = ConvNeXt_model()\n            model.head = nn.Linear(model.head.in_features, num_classes) \n            num=5\n            \n        elif model_type == 'vit':\n            model = ViT_model()\n            model.heads=nn.Linear(768,2)\n            num=13\n        else:\n            raise ValueError(\"O parâmetro 'model_type' deve ser 'convnext' ou 'vit'.\")\n\n        model = model.to(device)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n  \n        if model_type == 'convnext':\n            print(\"Experiment 5 - ConvNeXt (No Fine-Tuning and with RandAugment)\")\n        elif model_type == 'vit':\n            print(\"Experiment 5 - ViT (No Fine-Tuning and with RandAugment)\")    \n        \n        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,5, model, augmented_train_dataloader, test_dataloader,\n                                                                                   criterion, optimizer, scheduler, device, 1, False, num)\n        \n        if model_type == 'convnext':\n            model = ConvNeXt_model()\n            model.head = nn.Linear(model.head.in_features, num_classes)  \n            num=6\n            \n        elif model_type == 'vit':\n            model = ViT_model()\n            model.heads=nn.Linear(768,2)\n            num=14\n        else:\n            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n\n        model = model.to(device)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n        if model_type == 'convnext':\n            print(\"Experiment 6 - ConvNeXt (With Fine-Tuning and with RandAugment)\")\n        elif model_type == 'vit':\n            print(\"Experiment 6 - ViT (With Fine-Tuning and with RandAugment)\")\n        \n        \n        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,6, model, augmented_train_dataloader, test_dataloader,\n                                                                                   criterion, optimizer, scheduler, device, num_epochs, True, num) \n\n    elif scenario == 5:\n        augmented_train_dataloader = Auto_RandAugment_transform(model_type, train_indices, batch_size)\n        test_dataloader=ft_data_transform(model_type, data_fraction, batch_size)\n        \n        if model_type == 'convnext':\n            model = ConvNeXt_model()\n            model.head = nn.Linear(model.head.in_features, num_classes)  \n            num=7\n            \n        elif model_type == 'vit':\n            model = ViT_model()\n            model.heads=nn.Linear(768,2)\n            num=15\n        else:\n            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n\n        model = model.to(device)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n        if model_type == 'convnext':\n            print(\"Experiment 7 - ConvNeXt (No Fine-Tuning and with RandAugment and AutoAugment)\")\n        elif model_type == 'vit':\n            print(\"Experiment 7 - ViT (No Fine-Tuning and with RandAugment and AutoAugment)\")\n            \n        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,7, model, augmented_train_dataloader, test_dataloader,\n                                                                                   criterion, optimizer, scheduler, device, 1, False, num)\n      \n        # Select the model\n        if model_type == 'convnext':\n            model = ConvNeXt_model()\n            model.head = nn.Linear(model.head.in_features, num_classes) \n            num=8\n            \n        elif model_type == 'vit':\n            model = ViT_model()\n            model.heads=nn.Linear(768,2)\n            num=16\n        else:\n            raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n\n        model = model.to(device)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n        if model_type == 'convnext':\n            print(\"Experiment 8 - ConvNeXt (With Fine-Tuning and with RandAugment and AutoAugment)\")\n        elif model_type == 'vit':\n            print(\"Experiment 8 - ViT (With Fine-Tuning and with RandAugment and AutoAugment)\")\n\n        train_loss, train_accuracy, train_f1, test_loss, test_accuracy,test_f1 = train_model(model_type,8, model, augmented_train_dataloader, test_dataloader,\n                                                                                   criterion, optimizer, scheduler, device, num_epochs, True, num)\n    else:\n        raise ValueError(\"The 'model_type' parameter must be 'convnext' or 'vit'.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:26:22.067853Z","iopub.execute_input":"2024-11-20T15:26:22.068177Z","iopub.status.idle":"2024-11-20T15:26:22.097637Z","shell.execute_reply.started":"2024-11-20T15:26:22.068150Z","shell.execute_reply":"2024-11-20T15:26:22.096842Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## **3.4. Performance Evaluation**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_experiment_graphs(df, experiment_number):\n    experiment_df = df[df['Experimento'] == experiment_number]\n\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(experiment_df['Epoch'], experiment_df['Train F1'], marker='o')\n    plt.xlabel('Epoch')\n    plt.ylabel('Train F1')\n    plt.title(f'Experiment {experiment_number} - Train F1')\n    plt.tight_layout()\n    plt.show()\n    \n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(experiment_df['Epoch'], experiment_df['Train ACC'], marker='o')\n    plt.xlabel('Epoch')\n    plt.ylabel('Train Accuracy')\n    plt.title(f'Experiment {experiment_number} - Train Accuracy')\n    plt.tight_layout()\n    plt.show()\n    \n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(experiment_df['Epoch'], experiment_df['Train Loss'], marker='o')\n    plt.xlabel('Epoch')\n    plt.ylabel('Train Loss')\n    plt.title(f'Experimento {experiment_number} - Trai loss')\n    plt.tight_layout()\n    plt.show()\n    \n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(experiment_df['Epoch'], experiment_df['Test F1'], marker='o')\n    plt.xlabel('Epoch')\n    plt.ylabel('Test F1')\n    plt.title(f'Experiment {experiment_number} - Test F1')\n    plt.tight_layout()\n    plt.show()\n    \n   \n    plt.figure(figsize=(8, 6))\n    plt.plot(experiment_df['Epoch'], experiment_df['Test ACC'], marker='o')\n    plt.xlabel('Epoch')\n    plt.ylabel('Test Accuracy')\n    plt.title(f'Experiment {experiment_number} - Test Accuracy')\n    plt.tight_layout()\n    plt.show()\n    \n   \n    plt.figure(figsize=(8, 6))\n    plt.plot(experiment_df['Epoch'], experiment_df['Test Loss'], marker='o')\n    plt.xlabel('Epoch')\n    plt.ylabel('Test loss')\n    plt.title(f'Experimento {experiment_number} - Test loss')\n    plt.tight_layout()\n    plt.show()\n\ndef plot_general_graphs(df):\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n\n   \n    ax1 = axes[0, 0]\n    for experiment_number in df['Experimento'].unique():\n        experiment_df = df[df['Experimento'] == experiment_number]\n        ax1.plot(experiment_df['Epoch'], experiment_df['Train F1'], marker='o', label=f'Experimento {experiment_number}')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Train F1')\n    ax1.set_title('Overall - Train F1')\n    ax1.legend()\n\n   \n    ax2 = axes[0, 1]\n    for experiment_number in df['Experiment'].unique():\n        experiment_df = df[df['Experiment'] == experiment_number]\n        ax2.plot(experiment_df['Epoch'], experiment_df['Train Loss'], marker='o', label=f'Experiment {experiment_number}')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Train Loss')\n    ax2.set_title('Overall - Train Loss')\n    ax2.legend()\n\n   \n    ax3 = axes[1, 0]\n    for experiment_number in df['Experimento'].unique():\n        experiment_df = df[df['Experimento'] == experiment_number]\n        ax3.plot(experiment_df['Epoch'], experiment_df['Test F1'], marker='o', label=f'Experimento {experiment_number}')\n    ax3.set_xlabel('Epoch')\n    ax3.set_ylabel('Test F1')\n    ax3.set_title('Overall - Test F1')\n    ax3.legend()\n\n    \n    ax4 = axes[1, 1]\n    for experiment_number in df['Experiment'].unique():\n        experiment_df = df[df['Experiment'] == experiment_number]\n        ax4.plot(experiment_df['Epoch'], experiment_df['Test Loss'], marker='o', label=f'Experimento {experiment_number}')\n    ax4.set_xlabel('Epoch')\n    ax4.set_ylabel('Test Loss')\n    ax4.set_title('Overall - Test Loss')\n    ax4.legend()\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:26:39.348036Z","iopub.execute_input":"2024-11-20T15:26:39.348393Z","iopub.status.idle":"2024-11-20T15:26:39.361894Z","shell.execute_reply.started":"2024-11-20T15:26:39.348343Z","shell.execute_reply":"2024-11-20T15:26:39.360943Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\nrun_scenario('convnext',1,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:26:48.115301Z","iopub.execute_input":"2024-11-20T15:26:48.115993Z","iopub.status.idle":"2024-11-20T15:31:26.781619Z","shell.execute_reply.started":"2024-11-20T15:26:48.115966Z","shell.execute_reply":"2024-11-20T15:31:26.780764Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of training objects: 10000\nNumber of test objects: 2000\n============================================================================\nExperimento 1 - ConvNeXt (Sem Fine-Tuning)\n----------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"[Epoch 1]: 100%|██████████| 625/625 [01:53<00:00,  5.53it/s, loss=0.66] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.043394 | Train Accuracy: 52.08% | Train F1-Score: 0.469561\nTest Loss: 0.086170 | Test Accuracy: 54.65% | Test F1-Score: 0.497420\n\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"mean_df = df.groupby(['Modelo', 'Experimento']).mean()\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:31:26.783408Z","iopub.execute_input":"2024-11-20T15:31:26.783782Z","iopub.status.idle":"2024-11-20T15:31:26.798528Z","shell.execute_reply.started":"2024-11-20T15:31:26.783746Z","shell.execute_reply":"2024-11-20T15:31:26.797537Z"},"trusted":true},"outputs":[{"name":"stdout","text":"      Modelo  Experimento  Epoch  Train ACC  Train Loss  Train F1  Test ACC  \\\n11  convnext            1      1     0.5208    0.043394  0.469561    0.5465   \n\n    Test Loss  Test F1  \n11    0.08617  0.49742  \n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"run_scenario('convnext',2,0.1, num_epochs=3, batch_size=16, learning_rate=0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T16:47:59.677230Z","iopub.execute_input":"2024-11-20T16:47:59.678022Z","iopub.status.idle":"2024-11-20T17:02:08.464905Z","shell.execute_reply.started":"2024-11-20T16:47:59.677990Z","shell.execute_reply":"2024-11-20T17:02:08.463961Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of training objects: 10000\nNumber of test objects: 2000\n============================================================================\nExperiment 2 - ConvNeXt (With Fine-Tuning)\n----------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"[Epoch 1]: 100%|██████████| 625/625 [04:13<00:00,  2.47it/s, loss=0.261]   \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.007687 | Train Accuracy: 94.67% | Train F1-Score: 0.946699\nTest Loss: 0.001789 | Test Accuracy: 99.65% | Test F1-Score: 0.996494\n\n\n----------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"[Epoch 2]: 100%|██████████| 625/625 [03:43<00:00,  2.80it/s, loss=0.000504]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.000432 | Train Accuracy: 99.80% | Train F1-Score: 0.998000\nTest Loss: 0.003468 | Test Accuracy: 99.00% | Test F1-Score: 0.989975\n\n\n----------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"[Epoch 3]: 100%|██████████| 625/625 [03:43<00:00,  2.80it/s, loss=8.39e-5] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.000034 | Train Accuracy: 100.00% | Train F1-Score: 1.000000\nTest Loss: 0.000702 | Test Accuracy: 99.80% | Test F1-Score: 0.997997\n\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"mean_df = df.groupby(['Modelo', 'Experimento']).mean()\nprint(mean_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:30:59.354163Z","iopub.execute_input":"2024-11-20T17:30:59.354822Z","iopub.status.idle":"2024-11-20T17:30:59.366113Z","shell.execute_reply.started":"2024-11-20T17:30:59.354792Z","shell.execute_reply":"2024-11-20T17:30:59.365291Z"}},"outputs":[{"name":"stdout","text":"                      Epoch  Train ACC  Train Loss  Train F1  Test ACC  \\\nModelo   Experimento                                                     \nconvnext 1              1.0    0.52080    0.043394  0.469561    0.5465   \n         2              5.5    0.99447    0.000828  0.994470    0.9981   \n\n                      Test Loss   Test F1  \nModelo   Experimento                       \nconvnext 1             0.086170  0.497420  \n         2             0.000827  0.998097  \n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"#run_scenario('convnext',3,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T06:23:23.295748Z","iopub.execute_input":"2024-05-19T06:23:23.296175Z","iopub.status.idle":"2024-05-19T07:09:37.555452Z","shell.execute_reply.started":"2024-05-19T06:23:23.296143Z","shell.execute_reply":"2024-05-19T07:09:37.554535Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n#print(mean_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:15:00.210291Z","iopub.execute_input":"2024-05-19T07:15:00.210669Z","iopub.status.idle":"2024-05-19T07:15:00.224568Z","shell.execute_reply.started":"2024-05-19T07:15:00.210637Z","shell.execute_reply":"2024-05-19T07:15:00.223638Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#run_scenario('convnext',4,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T07:15:05.181599Z","iopub.execute_input":"2024-05-19T07:15:05.18225Z","iopub.status.idle":"2024-05-19T08:02:38.426958Z","shell.execute_reply.started":"2024-05-19T07:15:05.182221Z","shell.execute_reply":"2024-05-19T08:02:38.425988Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n#print(mean_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:04:53.164191Z","iopub.execute_input":"2024-05-19T08:04:53.164976Z","iopub.status.idle":"2024-05-19T08:04:53.177466Z","shell.execute_reply.started":"2024-05-19T08:04:53.164942Z","shell.execute_reply":"2024-05-19T08:04:53.176497Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#run_scenario('convnext',5,0.1, num_epochs=10, batch_size=16, learning_rate=0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T08:05:16.323828Z","iopub.execute_input":"2024-05-19T08:05:16.32468Z","iopub.status.idle":"2024-05-19T09:03:53.222356Z","shell.execute_reply.started":"2024-05-19T08:05:16.324642Z","shell.execute_reply":"2024-05-19T09:03:53.221414Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#mean_df = df.groupby(['Modelo', 'Experimento']).mean()\n#print(mean_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T09:05:53.554794Z","iopub.execute_input":"2024-05-19T09:05:53.555172Z","iopub.status.idle":"2024-05-19T09:05:53.568304Z","shell.execute_reply.started":"2024-05-19T09:05:53.555143Z","shell.execute_reply":"2024-05-19T09:05:53.567351Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv('metricas.csv', index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:31:20.284901Z","iopub.execute_input":"2024-11-20T17:31:20.285289Z","iopub.status.idle":"2024-11-20T17:31:20.291982Z","shell.execute_reply.started":"2024-11-20T17:31:20.285252Z","shell.execute_reply":"2024-11-20T17:31:20.290944Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"df.to_csv(\"/kaggle/working/results.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:33:24.508727Z","iopub.execute_input":"2024-11-20T17:33:24.509041Z","iopub.status.idle":"2024-11-20T17:33:24.515149Z","shell.execute_reply.started":"2024-11-20T17:33:24.509018Z","shell.execute_reply":"2024-11-20T17:33:24.514297Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import requests\n\nurl = \"https://www.kaggleusercontent.com/your-output-link.csv\"\nresponse = requests.get(url)\nwith open(\"results.csv\", \"wb\") as file:\n    file.write(response.content)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T17:34:28.172885Z","iopub.execute_input":"2024-11-20T17:34:28.173588Z","iopub.status.idle":"2024-11-20T17:34:28.327565Z","shell.execute_reply.started":"2024-11-20T17:34:28.173556Z","shell.execute_reply":"2024-11-20T17:34:28.326906Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}